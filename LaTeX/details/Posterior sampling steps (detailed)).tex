\documentclass[10pt, oneside]{article}   	% use "amsart" instead of "article" for AMSLaTeX format
\usepackage{geometry}                		% See geometry.pdf to learn the layout options. There are lots.
\geometry{letterpaper}                   		% ... or a4paper or a5paper or ... 
%\geometry{landscape}                		% Activate for rotated page geometry
%\usepackage[parfill]{parskip}    		% Activate to begin paragraphs with an empty line rather than an indent
\usepackage{graphicx}				% Use pdf, png, jpg, or epsÂ§ with pdflatex; use eps in DVI mode
								% TeX will automatically convert eps --> pdf in pdflatex		
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsfonts}
\usepackage{xcolor}
\usepackage{ulem}
\usepackage{cancel}
%SetFonts

%SetFonts
\newcommand{\Q}[1]{\textcolor{brown}{[Q: \textcolor{teal}{#1}]}}
\newcommand{\citea}[1]{\citeauthor{#1}, \href{cite.#1}{\textcolor{blue}{\citeyear{#1}}}}
\newcommand{\citeb}[1]{\citeauthor{#1} (\href{cite.#1}{\textcolor{blue}{\citeyear{#1}}})}
\newcommand{\citec}[2]{(\href{cite.#1}{\citeauthor{#1}}, \href{cite.#1}{\textcolor{blue} {\citeyear{#1}}}; \href{cite.#2}{\citeauthor{#2}}, \href{cite.#2}{\textcolor{blue} {\citeyear{#2}}})}
\newcommand{\todo}[1]{\textcolor{red}{[To Do: \textcolor{teal}{#1}]}}
\newcommand{\refa}[1]{\textcolor{blue}{\ref{#1}}}



\newcommand{\Ga}{\mbox{Ga}}
\renewcommand{\th}{\theta}
\newcommand{\cb}{\bm{c}}
\renewcommand{\sb}{\bm{s}}
\newcommand{\vb}{\bm{v}}
\newcommand{\wb}{\bm{w}}
\newcommand{\bx}{\mbox{\boldmath $x$}}
\newcommand{\beps}{\mbox{\boldmath $\epsilon$}}
\newcommand{\estimates}{\mathrel{\widehat{=}}}
\newcommand{\thh}{\widetilde{H}}
\newcommand{\scf}{\mathcal{F}}
\newcommand{\sx}{\mathcal{X}}
\newcommand{\sy}{\mathcal{Y}}
\newcommand{\bth}{\mbox{\boldmath $\theta$}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\R}{\mathbb{R}}
\renewcommand{\scf}{\mathcal{F}}
\renewcommand{\sx}{\mathcal{X}}
\renewcommand{\sy}{\mathcal{Y}}
\newcommand{\syy}{\mathbb{\sy}}
\newcommand{\sbb}{\mathcal{B}}
\newcommand{\sd}{\mathcal{D}}
\newcommand{\eps}{\epsilon}
\newcommand{\pr}{\mbox{Pr}}
\newcommand{\ub}{\mathbf{u}}
\newcommand{\tn}{{\widetilde n}}
\newcommand{\tmu}{\widetilde{\mu}}

\usepackage{amsthm} % for theorem environments

% Theorem styles
\newtheorem{result}{Result}[section] % Numbered within each section
\newtheorem{theorem}{Theorem}[section] % Numbered within each section
\newtheorem{lemma}[theorem]{Lemma} % Numbered with theorems
\newtheorem{corollary}[theorem]{Corollary} % Numbered with theorems

% Proof environment
\renewenvironment{proof}{\noindent \textit{Proof.}}

\title{Posterior Sampling Steps}
%\author{}
\date{}							% Activate to display a given date or no date

\begin{document}
\maketitle

\section{DP-SPGLM}
Final hierarchical model ---
\begin{align*}
 y_i \mid  z_i, x_i & \sim K(y_i \mid z_i); \\
z_i \mid x_i = x, \theta_{x}, \tmu & \sim p_x(z_i) \propto
                                       \exp(\th_{x} z_i) \widetilde
                                       \mu(z_i) \\ 
\theta_x \mid \beta, \tmu & \sim p(\theta_x \mid \widetilde \theta_x), \text{ with } {b^\prime}(\widetilde \theta_x) = g^{-1}(x^\prime \beta) = \lambda(x)  \\ 
\tmu & \sim \text{ gamma CRM}(\nu), \text{ with } \nu(ds, dz) = \frac{e^{-s}}{s} ds \cdot \alpha G_0(dz) \\ 
\beta & \sim \text{ MVN}(\mu_\beta, \Sigma_\beta) 
\end{align*}
where $ g_i(z) = \exp(\th_i z), \  \frac{\widetilde \mu}{T}  \sim DP(\alpha, G_0), \ K(\cdot \mid z_i)  = N(\cdot \mid z_i, \sigma^2_0)$ (or some other distribution). Here, if we assume $\theta_i$ does not depend on $\widetilde \mu$, we have a $(n?)$ non-homogeneous CRM(s?) case with $n$ (deterministic) perturbing functions $g_i, i=1, \dots, n$.
\newline Question: Should we make it more general and use normalized generalized gamma (NGG) as a prior? 
\subsection{Posterior sampling steps}
Parameters, $(\beta, \widetilde \mu)$ and latent variables,  $\{z_i\}_{i=1}^n, \{u_i\}_{i=1}^n$. 
\newline Choices to make for $\alpha, G_0, K, \sigma_0, \mu_\beta, \Sigma_\beta$. We can also put a prior on $\sigma_0$.
\subsubsection[]{$\beta$ (and $\theta$) update ---} 
$[\widetilde \beta \mid {\widetilde \mu}^{(t)},$---] $\sim N_p(\mu^\star_\beta, \Sigma^\star_\beta) 1_A\left(\widetilde{\beta}\right),$ where $\mu^\star_\beta = \beta^{(t)}, \Sigma^\star_\beta = \rho  \mathcal{I}^{-1}\left(\widehat \beta_{mle} ; \widehat {\widetilde \mu}_{mle}\right) \text{ or } \rho \mathcal{I}^{-1}\left(\beta^{(t)} ; {\widetilde \mu}^{(t)}\right)$, \newline where $A = \left\{\beta \in \R^p :  g^{-1}\left(x_i^T \beta \right) \in \sy \right \}$, $\rho \in (0,1]$ is a tuning parameter, and $\mathcal{I}^{-1}(\beta^{(t)} ; {\widetilde \mu}^{(t)})$ is the inverse Fisher information matrix associated with $\beta$ at $\beta = \beta^{(t)}$. That is,
\[\mathcal{I}\left(\beta^{(t)} ; {\widetilde \mu}^{(t)}\right) = \sum_{i=1}^n  \frac{x_i x_i^T}{\left(g^{\prime}\left(\mu^{(t)}_i\right)\right)^2 b^{\prime \prime}\left(\theta^{(t)}_i\right)} \ ,\]
$g^{\prime}\left(\mu^{(t)}_i\right) = g^{\prime}\left(g^{-1}\left(x_i^T \beta^{(t)}\right)\right)$, $b^{\prime \prime}\left(\theta^{(t)}_i\right) = {\sigma^{(t)}_i}^2$. Note that $b(\theta) = \ln T(\theta)$.
Writing ${\widetilde \mu}^{(t)}(\cdot) = \sum_{\ell} {\widetilde J}^{(t)}_\ell \delta_{{\widetilde z}^{(t)}_\ell}(\cdot)$ and hence, we solve for $\theta^{(t)}_i$ from $g^{-1}\left(x_i^T \beta^{(t)}\right) = \frac{\sum_{\ell} {\widetilde z}^{(t)}_\ell \exp \left(\theta^{(t)}_i {\widetilde z}^{(t)}_\ell\right) {\widetilde J}^{(t)}_\ell}{\sum_{\ell} \exp \left(\theta^{(t)}_i {\widetilde z}^{(t)}_\ell\right) {\widetilde J}^{(t)}_\ell}$. Omitting $(t)$ superscript from CRM for better readability, we have:
\[b^{\prime \prime}\left(\theta^{(t)}_i\right) = \frac{\sum_{\ell} \widetilde{z_\ell}^2 \exp \left(\theta^{(t)}_i \widetilde{z_\ell}\right)  \widetilde J_\ell}{\sum_{\ell} \exp \left(\theta^{(t)}_i \widetilde{z_\ell}\right)  \widetilde J_\ell} - \left[\frac{\sum_{\ell} \widetilde{z_\ell} \exp (\theta^{(t)}_i \widetilde{z_\ell})  \widetilde J_\ell}{\sum_{\ell} \exp (\theta^{(t)}_i \widetilde{z_\ell})  \widetilde J_\ell} \right]^2\ .\]

\subsubsection[]{$\widetilde \mu$ (also, $\{z_i\}_{i=1}^n, \{u_i\}_{i=1}^n$) updates ---} 
We assume $\th_i$ not depending on $\widetilde \mu$ and consider $\th_i = \th^{(t)}_i = \th(\beta^{(t+1)}, {\widetilde{\mu}}^{(t)}, x_i)$ to be fixed for simulating ${\widetilde \mu}^{(t+1)}$. 
We sample $\widetilde \mu$ from $[\widetilde \mu \mid \mathbf{y}, \mathbf{z}] \stackrel{d}{=}  [\widetilde \mu \mid \mathbf{z}]$, and $\mathbf{z}$ from $[\mathbf{z} \mid \widetilde \mu, \mathbf{y}]$. For simulating $\widetilde \mu$, we further introduce latent variables $u_1, \dots, u_n$ and sample $\widetilde \mu$ from $[\widetilde \mu \mid \mathbf{u}, \mathbf{z}]$ after we sample $\mathbf{u}$ from $[\mathbf{u} \mid \mathbf{z}]$.
\begin{enumerate}
\item[1.] $[\mathbf{u} \mid \mathbf{z}^{(t)}]:$
$\Pr\left(\ub \mid  \mathbf{z}^{(t)} \right) \propto e^{-\psi(\ub)} \prod_{i=1}^n \int_{\R^{+}} s e^{-s \left\{\sum_{i=1}^n u_i \exp(\th^{(t)}_i  z^{(t)}_i)\right\}} \rho(ds \mid z^{(t)}_i)$, with 
\begin{eqnarray*}
e^{-\psi(\ub)} & = & \exp \left \{ - \int_{\R^{+} \times \sy}  \left[1 - e^{- \left\{\sum_{i=1}^n u_i \exp(\th^{(t)}_i  z) \right\} s} \right] \nu(ds, dz) \right\} \\
& = & \exp \left \{ - \int_{\sy} \int_{\R^+}  \left[1 - e^{- \left\{\sum_{i=1}^n u_i \exp(\th^{(t)}_i  z) \right\} s} \right]\alpha \frac{e^{-s}}{s}ds G_0(dz) \right\}\\
& = & \exp \left \{ - \alpha \int_{\sy} \ln \left[1+ \sum_{i=1}^n u_i \exp(\th^{(t)}_i  z) \right] G_0(dz) \right\}\\
\end{eqnarray*}
Similarly, $\prod_{j=1}^n \int_{\R^{+}} s e^{-s \left\{\sum_{i=1}^n u_i \exp(\th^{(t)}_i  z^{(t)}_j)\right\}} \rho(ds \mid z^{(t)}_j) = \cdots  \frac{e^{-s}}{s}ds = \prod_{j=1}^n \int_{\R^{+}}  e^{-s \left\{1+ \sum_{i=1}^n u_i \exp(\th^{(t)}_i  z^{(t)}_j)\right\}} ds =  \prod_{j=1}^n \frac{1}{\left\{1+ \sum_{i=1}^n u_i \exp(\th^{(t)}_i  z^{(t)}_j)\right\}}  = \exp\left[- \sum_{j=1}^n \ln \left\{ 1+ \sum_{i=1}^n u_i \exp(\th^{(t)}_i  z^{(t)}_j)\right\}  \right]$. Hence,
\begin{eqnarray*}
Pr\left(\ub \mid  \mathbf{z}^{(t)} \right) & \propto & \exp \left \{ - \int_{\sy} \ln \left[1+ \sum_{i=1}^n u_i \exp(\th^{(t)}_i  z) \right] \left(\alpha G_0(dz) + \sum_{j=1}^n \delta_{z^{(t)}_j}(dz)\right) \right\} \\
& \propto & \exp \left \{ - \int_{\sy} \ln \left[1+ \sum_{i=1}^n u_i \exp(\th^{(t)}_i  z) \right] G_n(dz) \right\} 
\end{eqnarray*}
where $G_n(\cdot) = \alpha G_0(\cdot) +  \sum_{j=1}^n \delta_{z^{(t)}_j} (\cdot)$. The second term can also be written as $\sum_{\ell = 1}^{\widetilde n} n_\ell \delta_{z^\star_\ell}(\cdot)$, where $\{z^\star_\ell\}_{\ell=1}^{\widetilde n}$ are the unique values, among $\{z^{(t)}_j, j=1, \dots, n\}$, with $\{n_\ell\}_{\ell=1}^{\widetilde n}$ ties.

Three possible approaches:

1. We generate $u_j \sim \Pr(u_j \mid u^{(t+1)}_{1:j-1}, u^{(t)}_{j+1:n}, \mathbf{z}^{(t)}), j = 1, \dots, n$. In practice, we generate from an exact distribution (calculation depends on the choice of $G_0$) or in case of difficulty, we can always approximate it using Monte Carlo,
$\approx \exp\left(- (1/R)\sum_{r=1}^R \Psi(z_r; u_j)\right), \quad z_r \sim G_n$, where $\Psi(z_r; u_j) = \ln \left[1+ \sum_{i=1}^{j-1} u^{(t+1)}_i \exp(\th^{(t)}_i  z_r) + \sum_{i=j+1}^{n} u^{(t)}_i \exp(\th^{(t)}_i  z_r) + u_j \exp(\th^{(t)}_j  z_r) \right]$. We can then followup with an MH acceptance-rejection step.

2. [Idea from Igor's Stat Sci 2013 paper] We generate proposal using random walk, $u_j \sim Gamma\left(\delta, \delta\big/u^{(t)}_j\right), j = 1, \dots, n$ and followed up with an MH acceptance-rejection step. The hyperparameter $\delta (\geq 1)$ controls the acceptance rate of the M-H step being higher for larger values.

3. Should we generate $v = \sum_{i=1}^n u_i \exp(\th^{(t)}_i  z)$ and then use that somehow to generate $u_j$'s?  Think about proposal: some transformation eg., log? something more clever?

Note: $P(u_j, T_j) = T_j e^{-u_j T_j} P(T_j) \implies$ the prior on $u_j \mid T_j$ is gamma$(1, T_j)$. Hence for initializing $u_j$, we generate from Gamma(1, $T_j$).

\item[2.] $[\widetilde \mu \mid \mathbf{u}^{(t+1)}, \mathbf{z}^{(t)}]:$
\begin{itemize}
 \item[(a)]  ${\widetilde \mu}^{\left({\ub}^{(t+1)}\right)} \sim \text{ CRM}\left(\nu^{(\ub)}\right) \text{ with } \nu^{(\ub)} (ds, dz) = e^{- \left\{\sum_{i=1}^n u^{(t+1)}_i \exp(\th^{(t+1)}_i z) \right\} s} \rho(ds \mid z) G_0(dz) = e^{- \Psi(z) s} \rho(ds \mid z) G_0(dz) = \alpha \frac{e^{- \left(\Psi(z) +1\right) s}}{s} ds G_0(dz)$.  ${\widetilde \mu}^{\left({\ub}^{(t+1)}\right)}(\cdot) = \sum_{\ell =1}^M J^{(t+1)}_\ell \delta_{z^{(t+1)}_\ell}(\cdot)$ (random locations $z_\ell$ and jumps $J_\ell$ are generated using Ferguson and Klass (1972) algorithm; $M$ is used for finite approximation of the CRM). Omitting superscript $u$ for better readability.
 $$
 N(v) = \nu([v, \infty], \sy) = \int_v^\infty  \int_{\sy} \nu(ds, dz) = \alpha \int_v^\infty  \int_{\sy} \frac{e^{- \left(\Psi(z) +1\right) s}}{s} ds G_0(dz) 
 $$
 
 \item[(b)] $\Pr\left(J^{\left({\ub}^{(t+1)}, z^{(t)}_j\right)}_j\right) \propto s e^{-s \left\{\sum_{i=1}^n u^{(t+1)}_i \exp(\th^{(t+1)}_i z^{(t)}_j)\right\}} \rho(ds \mid z^{(t)}_j) =  e^{- \left(\Psi\left(z^{(t)}_j\right) +1\right) s} \sim EXP$ $\left(mean = 1\bigg/\left(\Psi\left(z^{(t)}_j\right) +1\right)\right)$. Let's assume we have $r$ unique fixed locations ($z^\star_1, \dots, z^\star_r$). Then for jumps, we generate from $\Pr(J^\star_\ell \mid -)\propto s^{n_\ell - 1}  e^{- \left(\Psi\left({z^\star_\ell}^{(t)}\right) +1\right) s} \equiv Gamma\left(n_\ell,  \Psi\left({z^\star_\ell}^{(t)}\right) +1\right)$ $, \ell = 1, \dots, r$.
\end{itemize}
\item[3.] $[\mathbf{z} \mid {\widetilde \mu}^{(t+1)}, \mathbf{y}]:$ 
\begin{eqnarray*}
\Pr[z_i \mid {\widetilde \mu}^{(t+1)}, \mathbf{y}] \propto \sum_{\ell} K(y_i \mid z_i) \exp(\theta_i z_i) {\widetilde{J}}^{(t+1)}_\ell \delta_{{\widetilde z}^{(t+1)}_\ell}(z_i)\ ,
\end{eqnarray*} 
where $\widetilde J := \{{\widetilde{J}}^{(t+1)}_\ell \}_{\ell \geq 1} = \{{J^\star_1}^{(t+1)}, \dots,  {J^\star_r}^{(t+1)}, {J_1}^{(t+1)}, \dots, \}$ [first $r$ jumps are from 2(b) and rest from 2(a)]. Similarly, $\widetilde Z := \{{\widetilde z}^{(t+1)}_\ell\}_{\ell \geq 1} = \{{z^\star_1}^{(t+1)}, \dots,  {z^\star_r}^{(t+1)}, {z_1}^{(t+1)}, \dots, \}$. Hence, $z_i \sim a \ categorical \ distribution$.
\item[4.] (Optional) Resample fixed locations $z_i (or, \ z^\star_\ell)$ to reduce `sticky clusters effect'. 
\end{enumerate}

\end{document}